#!/usr/bin/env python3
"""
–ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–¥–µ–∫—Å–æ–≤ –†–§

–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ PDF —Ñ–∞–π–ª—ã –∫–æ–¥–µ–∫—Å–æ–≤ –∏–∑ –≤—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏,
—Å–æ–∑–¥–∞—ë—Ç Markdown –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏ JSON —Ñ–∞–π–ª—ã –¥–ª—è ML/RAG —Å–∏—Å—Ç–µ–º.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python pipeline_codex.py --input data/input/codex --output data/output

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - PDF —Ñ–∞–π–ª—ã –∫–æ–¥–µ–∫—Å–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ input/
    - –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (pip install -e .)
"""

import sys
import argparse
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
import json
import time

# –ò–º–ø–æ—Ä—Ç –ø–∞—Ä—Å–µ—Ä–∞
sys.path.insert(0, str(Path(__file__).parent))
from src import NPAParser
from src.utils.logger import logger


class CodexPipeline:
    """
    –ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–¥–µ–∫—Å–æ–≤ –†–§

    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö PDF –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    - –ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞
    - –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É
    - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ —Å –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º —Ä–∞–±–æ—Ç—ã
    - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á—ë—Ç–∞
    """

    def __init__(
        self,
        input_dir: Path,
        output_dir: Path,
        use_markdown_mode: bool = True,
        clean_text: bool = True
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞

        Args:
            input_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å PDF —Ñ–∞–π–ª–∞–º–∏ –∫–æ–¥–µ–∫—Å–æ–≤
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            use_markdown_mode: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pymupdf4llm –¥–ª—è Markdown
            clean_text: –û—á–∏—â–∞—Ç—å —Ç–µ–∫—Å—Ç –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.use_markdown_mode = use_markdown_mode
        self.clean_text = clean_text

        # –°–æ–∑–¥–∞—ë–º –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.markdown_dir = self.output_dir / "markdown"
        self.json_dir = self.output_dir / "json"
        self.reports_dir = self.output_dir / "reports"

        for dir_path in [self.markdown_dir, self.json_dir, self.reports_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞
        self.parser = NPAParser(
            use_markdown_mode=use_markdown_mode,
            clean_text=clean_text
        )

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.stats = {
            "total_files": 0,
            "processed": 0,
            "failed": 0,
            "skipped": 0,
            "start_time": None,
            "end_time": None,
            "documents": []
        }

        logger.info(f"–ü–∞–π–ø–ª–∞–π–Ω –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {input_dir} -> {output_dir}")

    def find_pdf_files(self) -> List[Path]:
        """
        –ü–æ–∏—Å–∫ –≤—Å–µ—Ö PDF —Ñ–∞–π–ª–æ–≤ –≤–æ –≤—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏

        Returns:
            List[Path]: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ PDF —Ñ–∞–π–ª–∞–º
        """
        pdf_files = list(self.input_dir.glob("*.pdf"))
        pdf_files.extend(self.input_dir.glob("**/*.pdf"))

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        pdf_files = list(set(pdf_files))
        pdf_files.sort()

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ PDF —Ñ–∞–π–ª–æ–≤: {len(pdf_files)}")
        return pdf_files

    def process_document(self, pdf_path: Path) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Args:
            pdf_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É

        Returns:
            Dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        doc_stats = {
            "filename": pdf_path.name,
            "status": "unknown",
            "error": None,
            "doc_type": None,
            "number": None,
            "chapters": 0,
            "articles": 0,
            "processing_time": 0,
            "markdown_size": 0,
            "json_size": 0
        }

        start_time = time.time()

        try:
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {pdf_path.name}")

            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document = self.parser.parse(str(pdf_path))

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            output_name = pdf_path.stem

            # –≠–∫—Å–ø–æ—Ä—Ç –≤ Markdown (–ü–†–ò–û–†–ò–¢–ï–¢ #1)
            markdown_path = self.markdown_dir / f"{output_name}.md"
            document.export_markdown(str(markdown_path))

            # –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON
            json_path = self.json_dir / f"{output_name}.json"
            document.export_json(str(json_path))

            # –°–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            doc_stats.update({
                "status": "success",
                "doc_type": document.metadata.doc_type,
                "number": document.metadata.number,
                "chapters": len(document.chapters),
                "articles": len(document.articles) if document.articles else sum(
                    len(ch.articles) for ch in document.chapters
                ),
                "processing_time": time.time() - start_time,
                "markdown_size": markdown_path.stat().st_size,
                "json_size": json_path.stat().st_size
            })

            logger.success(
                f"‚úÖ {pdf_path.name}: "
                f"{doc_stats['doc_type']} N {doc_stats['number']}, "
                f"{doc_stats['chapters']} –≥–ª–∞–≤, {doc_stats['articles']} —Å—Ç–∞—Ç–µ–π"
            )

        except Exception as e:
            doc_stats["status"] = "failed"
            doc_stats["error"] = str(e)
            doc_stats["processing_time"] = time.time() - start_time

            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {pdf_path.name}: {e}")

        return doc_stats

    def run(self) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Returns:
            Dict: –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        print("=" * 80)
        print("üöÄ –ü–ê–ô–ü–õ–ê–ô–ù –û–ë–†–ê–ë–û–¢–ö–ò –ö–û–î–ï–ö–°–û–í –†–§")
        print("=" * 80)
        print(f"–í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.input_dir}")
        print(f"–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.output_dir}")
        print(f"Markdown —Ä–µ–∂–∏–º: {'–î–∞' if self.use_markdown_mode else '–ù–µ—Ç'}")
        print(f"–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞: {'–î–∞' if self.clean_text else '–ù–µ—Ç'}")
        print("=" * 80)
        print()

        # –ü–æ–∏—Å–∫ PDF —Ñ–∞–π–ª–æ–≤
        pdf_files = self.find_pdf_files()

        if not pdf_files:
            logger.warning("‚ö†Ô∏è  PDF —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            print("\n‚ö†Ô∏è  PDF —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏:")
            print(f"   {self.input_dir}")
            print("\n–ü–æ–º–µ—Å—Ç–∏—Ç–µ PDF —Ñ–∞–π–ª—ã –∫–æ–¥–µ–∫—Å–æ–≤ –≤ —ç—Ç—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–æ–≤–∞.")
            return self.stats

        self.stats["total_files"] = len(pdf_files)
        self.stats["start_time"] = datetime.now()

        print(f"üìö –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(pdf_files)}")
        print()

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        for i, pdf_path in enumerate(pdf_files, 1):
            print(f"[{i}/{len(pdf_files)}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {pdf_path.name}")

            doc_stats = self.process_document(pdf_path)
            self.stats["documents"].append(doc_stats)

            if doc_stats["status"] == "success":
                self.stats["processed"] += 1
                print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ ({doc_stats['processing_time']:.1f}s)")
            elif doc_stats["status"] == "failed":
                self.stats["failed"] += 1
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {doc_stats['error']}")

            print()

        self.stats["end_time"] = datetime.now()

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞
        self._generate_report()

        # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self._print_summary()

        return self.stats

    def _generate_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞ –æ –æ–±—Ä–∞–±–æ—Ç–∫–µ"""
        report_path = self.reports_dir / f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(
                {
                    **self.stats,
                    "start_time": self.stats["start_time"].isoformat(),
                    "end_time": self.stats["end_time"].isoformat(),
                    "total_time": (self.stats["end_time"] - self.stats["start_time"]).total_seconds()
                },
                f,
                ensure_ascii=False,
                indent=2
            )

        logger.info(f"–û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {report_path}")

    def _print_summary(self):
        """–í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        total_time = (self.stats["end_time"] - self.stats["start_time"]).total_seconds()

        print("=" * 80)
        print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("=" * 80)
        print(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤:        {self.stats['total_files']}")
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ:  {self.stats['processed']} ‚úÖ")
        print(f"–û—à–∏–±–æ–∫:              {self.stats['failed']} ‚ùå")
        print(f"–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:     {total_time:.1f}s ({total_time/60:.1f} –º–∏–Ω)")

        if self.stats["processed"] > 0:
            avg_time = total_time / self.stats["processed"]
            print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è/–¥–æ–∫:   {avg_time:.1f}s")

        print()
        print("üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print(f"   Markdown: {self.markdown_dir}")
        print(f"   JSON:     {self.json_dir}")
        print(f"   –û—Ç—á—ë—Ç—ã:   {self.reports_dir}")
        print()

        # –î–µ—Ç–∞–ª–∏ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        if self.stats["processed"] > 0:
            print("üìö –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:")
            for doc in self.stats["documents"]:
                if doc["status"] == "success":
                    print(f"   ‚úÖ {doc['filename']}: {doc['articles']} —Å—Ç–∞—Ç–µ–π, "
                          f"{doc['markdown_size']/1024:.1f} KB (MD)")

        if self.stats["failed"] > 0:
            print()
            print("‚ùå –û—à–∏–±–∫–∏:")
            for doc in self.stats["documents"]:
                if doc["status"] == "failed":
                    print(f"   ‚Ä¢ {doc['filename']}: {doc['error']}")

        print("=" * 80)


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω"""
    parser = argparse.ArgumentParser(
        description="–ü–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–¥–µ–∫—Å–æ–≤ –†–§ –¥–ª—è ML/RAG —Å–∏—Å—Ç–µ–º",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

  # –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
  python pipeline_codex.py

  # –£–∫–∞–∑–∞—Ç—å –≤—Ö–æ–¥–Ω—É—é –∏ –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
  python pipeline_codex.py --input data/input/codex --output data/output

  # –û—Ç–∫–ª—é—á–∏—Ç—å Markdown —Ä–µ–∂–∏–º
  python pipeline_codex.py --no-markdown-mode

  # –û—Ç–∫–ª—é—á–∏—Ç—å –æ—á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞
  python pipeline_codex.py --no-clean-text

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π:
  data/
    input/
      codex/              <- –°—é–¥–∞ –ø–æ–º–µ—Å—Ç–∏—Ç–µ PDF —Ñ–∞–π–ª—ã –∫–æ–¥–µ–∫—Å–æ–≤
    output/
      markdown/           <- Markdown –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π (–ü–†–ò–û–†–ò–¢–ï–¢ –¥–ª—è RAG)
      json/               <- JSON —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
      reports/            <- –û—Ç—á—ë—Ç—ã –æ –æ–±—Ä–∞–±–æ—Ç–∫–µ
        """
    )

    parser.add_argument(
        "--input",
        type=Path,
        default=Path("data/input/codex"),
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å PDF —Ñ–∞–π–ª–∞–º–∏ –∫–æ–¥–µ–∫—Å–æ–≤ (default: data/input/codex)"
    )

    parser.add_argument(
        "--output",
        type=Path,
        default=Path("data/output"),
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (default: data/output)"
    )

    parser.add_argument(
        "--no-markdown-mode",
        action="store_true",
        help="–û—Ç–∫–ª—é—á–∏—Ç—å pymupdf4llm (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ PyMuPDF)"
    )

    parser.add_argument(
        "--no-clean-text",
        action="store_true",
        help="–û—Ç–∫–ª—é—á–∏—Ç—å –æ—á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞ –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"
    )

    args = parser.parse_args()

    # –°–æ–∑–¥–∞—ë–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
    pipeline = CodexPipeline(
        input_dir=args.input,
        output_dir=args.output,
        use_markdown_mode=not args.no_markdown_mode,
        clean_text=not args.no_clean_text
    )

    try:
        stats = pipeline.run()

        # –ö–æ–¥ –≤—ã—Ö–æ–¥–∞: 0 –µ—Å–ª–∏ –≤—Å–µ —É—Å–ø–µ—à–Ω–æ, 1 –µ—Å–ª–∏ –±—ã–ª–∏ –æ—à–∏–±–∫–∏
        exit_code = 0 if stats["failed"] == 0 else 1
        sys.exit(exit_code)

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        logger.warning("–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(130)

    except Exception as e:
        print(f"\n\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        logger.exception("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ")
        sys.exit(1)


if __name__ == "__main__":
    main()
