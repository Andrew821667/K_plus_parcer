#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ–Ω–ª–∞–π–Ω.consultant.ru

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç curl-cffi –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã –æ—Ç –±–æ—Ç–æ–≤.
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
"""

import asyncio
from pathlib import Path
from curl_cffi import requests as curl_requests
from bs4 import BeautifulSoup


async def fetch_page(url: str, session=None) -> tuple[str, str]:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º curl-cffi

    Returns:
        (html, error_message)
    """
    try:
        print(f"üì° –ó–∞–ø—Ä–æ—Å: {url}")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º curl-cffi —Å –∏–º–∏—Ç–∞—Ü–∏–µ–π Chrome –±—Ä–∞—É–∑–µ—Ä–∞
        response = curl_requests.get(
            url,
            impersonate="chrome110",  # –ò–º–∏—Ç–∞—Ü–∏—è Chrome 110
            timeout=10
        )

        print(f"   ‚úÖ –°—Ç–∞—Ç—É—Å: {response.status_code}")

        if response.status_code == 200:
            return response.text, None
        else:
            return None, f"HTTP {response.status_code}"

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        return None, str(e)


def analyze_login_form(html: str):
    """–ê–Ω–∞–ª–∏–∑ —Ñ–æ—Ä–º—ã –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"""
    soup = BeautifulSoup(html, 'lxml')

    print("\nüîç –ê–ù–ê–õ–ò–ó –§–û–†–ú–´ –ê–í–¢–û–†–ò–ó–ê–¶–ò–ò:")
    print("=" * 60)

    # –ò—â–µ–º –≤—Å–µ —Ñ–æ—Ä–º—ã
    forms = soup.find_all('form')
    print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–æ—Ä–º: {len(forms)}")

    for i, form in enumerate(forms, 1):
        print(f"\n–§–æ—Ä–º–∞ #{i}:")
        print(f"  Action: {form.get('action', 'N/A')}")
        print(f"  Method: {form.get('method', 'N/A')}")
        print(f"  ID: {form.get('id', 'N/A')}")
        print(f"  Class: {form.get('class', 'N/A')}")

        # –ò—â–µ–º –ø–æ–ª—è –≤–≤–æ–¥–∞
        inputs = form.find_all('input')
        if inputs:
            print(f"  –ü–æ–ª—è –≤–≤–æ–¥–∞ ({len(inputs)}):")
            for inp in inputs:
                print(f"    - name='{inp.get('name', 'N/A')}', "
                      f"type='{inp.get('type', 'N/A')}', "
                      f"id='{inp.get('id', 'N/A')}'")

    # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é/–≤—Ö–æ–¥
    print("\nüîó –°–°–´–õ–ö–ò –ù–ê –ê–í–¢–û–†–ò–ó–ê–¶–ò–Æ:")
    login_keywords = ['login', 'auth', 'signin', '–≤—Ö–æ–¥', '–≤–æ–π—Ç–∏']
    for link in soup.find_all('a', href=True):
        href = link.get('href', '')
        text = link.get_text(strip=True).lower()

        if any(keyword in href.lower() or keyword in text for keyword in login_keywords):
            print(f"  {text}: {href}")


def analyze_search(html: str):
    """–ê–Ω–∞–ª–∏–∑ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞"""
    soup = BeautifulSoup(html, 'lxml')

    print("\nüîé –ê–ù–ê–õ–ò–ó –ü–û–ò–°–ö–ê:")
    print("=" * 60)

    # –ò—â–µ–º —Ñ–æ—Ä–º—ã –ø–æ–∏—Å–∫–∞
    search_forms = []
    for form in soup.find_all('form'):
        action = str(form.get('action', '')).lower()
        if 'search' in action or 'req' in action:
            search_forms.append(form)

    # –ò–ª–∏ –∏—â–µ–º input —Å —Ç–∏–ø–æ–º search
    search_inputs = soup.find_all('input', {'type': 'search'})
    search_inputs += soup.find_all('input', {'name': lambda x: x and 'search' in x.lower()})

    print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–æ—Ä–º –ø–æ–∏—Å–∫–∞: {len(search_forms)}")
    print(f"–ù–∞–π–¥–µ–Ω–æ –ø–æ–ª–µ–π –ø–æ–∏—Å–∫–∞: {len(search_inputs)}")

    for i, form in enumerate(search_forms, 1):
        print(f"\n–§–æ—Ä–º–∞ –ø–æ–∏—Å–∫–∞ #{i}:")
        print(f"  Action: {form.get('action')}")
        print(f"  Method: {form.get('method')}")

        for inp in form.find_all('input'):
            print(f"    - name='{inp.get('name')}', type='{inp.get('type')}'")


def save_html(html: str, filename: str):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å HTML –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    output_dir = Path("research_output")
    output_dir.mkdir(exist_ok=True)

    filepath = output_dir / filename
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(html)

    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filepath}")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
    print("=" * 80)
    print("üî¨ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –°–¢–†–£–ö–¢–£–†–´ –û–ù–õ–ê–ô–ù.CONSULTANT.RU")
    print("=" * 80)
    print()

    urls_to_check = [
        ("–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞", "https://online.consultant.ru"),
        ("–ì–ª–∞–≤–Ω–∞—è (CGI)", "https://online.consultant.ru/cgi/online.cgi?req=home"),
        ("–ü–æ–∏—Å–∫", "https://online.consultant.ru/cgi/online.cgi?req=doc"),
    ]

    for name, url in urls_to_check:
        print(f"\n{'='*80}")
        print(f"üìÑ {name}")
        print(f"{'='*80}")

        html, error = await fetch_page(url)

        if html:
            # –ê–Ω–∞–ª–∏–∑
            analyze_login_form(html)
            analyze_search(html)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            filename = f"{name.lower().replace(' ', '_')}.html"
            save_html(html, filename)
        else:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É: {error}")

        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        await asyncio.sleep(2)

    print("\n" + "="*80)
    print("‚úÖ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print("="*80)
    print()
    print("üìÅ HTML —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: ./research_output/")
    print()
    print("üîç –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
    print("1. –û—Ç–∫—Ä–æ–π—Ç–µ HTML —Ñ–∞–π–ª—ã –≤ –±—Ä–∞—É–∑–µ—Ä–µ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
    print("2. –ò–∑—É—á–∏—Ç–µ —Ñ–æ—Ä–º—ã –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–∞")
    print("3. –ò–ª–∏ –æ—Ç–∫—Ä–æ–π—Ç–µ —Å–∞–π—Ç –≤ –±—Ä–∞—É–∑–µ—Ä–µ —Å DevTools (F12)")
    print("   - –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞—Ç—å—Å—è")
    print("   - –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ –≤–∫–ª–∞–¥–∫–µ Network –∫–∞–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è")
    print("   - –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∏–º–µ–Ω–∞ –ø–æ–ª–µ–π —Ñ–æ—Ä–º—ã")
    print()


if __name__ == "__main__":
    asyncio.run(main())
